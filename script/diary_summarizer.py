#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ—¥è®°æ–‡ä»¶è§£æå’Œæ—¶é—´çº¿ç”Ÿæˆå·¥å…·

æ­¤è„šæœ¬å°†æ‰«ææŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„Markdownæ ¼å¼æ—¥è®°æ–‡ä»¶ï¼Œæå–æ¯å¤©çš„å…³é”®å†…å®¹å’Œæ´»åŠ¨ï¼Œ
ç”ŸæˆåŒ…å«å¿ƒæƒ…æŒ‡æ•°ã€æ‘˜è¦å’Œå…³é”®è¯çš„JSONæ ¼å¼æ•°æ®ï¼Œå¯å¯¼å…¥åˆ°æ—¶é—´çº¿ç»„ä»¶ã€‚
"""

import os
import re
import json
import argparse
from datetime import datetime
import glob
from pathlib import Path
import random
import subprocess
import requests
import tempfile

# å°è¯•å¯¼å…¥OpenAIåŒ…ï¼Œç”¨äºæ‘˜è¦ç”Ÿæˆ
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("è­¦å‘Š: OpenAI åŒ…æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„æ–‡æœ¬æå–æ–¹æ³•ã€‚å¦‚éœ€æ›´å¥½çš„æ‘˜è¦æ•ˆæœï¼Œè¯·å®‰è£…: pip install openai")

# å°è¯•æ£€æµ‹Ollamaå¯ç”¨æ€§
try:
    response = requests.get("http://localhost:11434/api/version", timeout=2)
    OLLAMA_AVAILABLE = response.status_code == 200
except:
    OLLAMA_AVAILABLE = False
    print("è­¦å‘Š: OllamaæœåŠ¡ä¸å¯ç”¨ã€‚å¦‚æœå·²å®‰è£…Ollamaï¼Œè¯·ç¡®ä¿æœåŠ¡æ­£åœ¨è¿è¡Œã€‚")

# è®¾ç½®é¢œè‰²åˆ—è¡¨ï¼Œç”¨äºæ—¶é—´çº¿äº‹ä»¶
COLORS = ['blue', 'green', 'red', 'orange', 'purple', 'cyan', 'pink', 'amber', 'indigo', 'teal']

# å¿ƒæƒ…çº§åˆ«æ˜ å°„
MOOD_LEVELS = {
    "æœ‰æ”¶è·å¾ˆå¼€å¿ƒ": {"level": 5, "emoji": "ğŸ˜„", "color": "green"},
    "ä¸€èˆ¬": {"level": 4, "emoji": "ğŸ™‚", "color": "blue"},
    "ä¸è¾¾æ ‡": {"level": 3, "emoji": "ğŸ˜", "color": "orange"},
    "ç³Ÿç³•": {"level": 2, "emoji": "ğŸ˜", "color": "red"},
    "æ— è®°å½•": {"level": 1, "emoji": "â“", "color": "grey"}
}

def extract_date_from_filename(filename):
    """ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸ"""
    # å°è¯•ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸ (æ ¼å¼å¯èƒ½ä¸º: YYYY-MM-DD æˆ– YYYYMMDD)
    date_patterns = [
        r'(\d{4}-\d{2}-\d{2})',  # YYYY-MM-DD
        r'(\d{4}\d{2}\d{2})',     # YYYYMMDD
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, filename)
        if match:
            date_str = match.group(1)
            # ç»Ÿä¸€æ—¥æœŸæ ¼å¼ä¸º YYYY.MM.DD
            if len(date_str) == 8:  # YYYYMMDD
                return f"{date_str[:4]}.{date_str[4:6]}.{date_str[6:]}"
            else:  # YYYY-MM-DD
                return date_str.replace('-', '.')
    
    # å¦‚æœæ–‡ä»¶åä¸­æ²¡æœ‰æ—¥æœŸï¼Œå°è¯•ä»æ–‡ä»¶ä¿®æ”¹æ—¶é—´è·å–
    try:
        mtime = os.path.getmtime(filename)
        date_obj = datetime.fromtimestamp(mtime)
        return date_obj.strftime("%Y.%m.%d")
    except:
        return None

def extract_last_modified_date(content):
    """ä»å†…å®¹ä¸­æå–æœ€è¿‘ä¿®æ”¹æ—¥æœŸ"""
    modified_pattern = r'æœ€è¿‘ä¿®æ”¹äº\s+(\d{4}-\d{2}-\d{2})'
    match = re.search(modified_pattern, content)
    if match:
        date_str = match.group(1)
        return date_str.replace('-', '.')
    return None

def clean_markdown(content):
    """æ¸…ç†Markdownå†…å®¹ï¼Œç§»é™¤å›¾ç‰‡ã€é“¾æ¥ç­‰æ ‡è®°ï¼Œä¹Ÿå»é™¤æ¨¡æ¿å†…å®¹"""
    # ç§»é™¤å›¾ç‰‡æ ‡è®°
    content = re.sub(r'!\[\[.*?\]\]', '', content)
    # ç§»é™¤å†…éƒ¨é“¾æ¥
    content = re.sub(r'\[\[(.*?)\]\]', r'\1', content)
    # ç§»é™¤ä»»åŠ¡åˆ—è¡¨æ ‡è®°ï¼Œä½†ä¿ç•™æ–‡æœ¬å†…å®¹
    content = re.sub(r'- \[ \]', '-', content)
    content = re.sub(r'- \[x\]', '- âœ“', content)
    # ç§»é™¤æ¨¡æ¿æ–‡æœ¬éƒ¨åˆ†
    content = re.sub(r'`è¯·åœ¨æ—©èµ·åç›´æ¥æŠ•å…¥å‰ä¸€å¤©å®šä¸‹æ¥çš„ä¸€ä¸ªä¼˜å…ˆçº§å·¥ä½œ`\s*`åœ¨ä¸€å¤©ç»“æŸä¹‹åå®šä¹‰ä¸€ä¸ªæ˜å¤©çš„å¾…åŠ`', '', content)
    # ç§»é™¤é‡å¤çš„åˆ†éš”çº¿
    content = re.sub(r'---+\s*---+', '---', content)
    # ç§»é™¤å›¾ç‰‡å¼•ç”¨
    content = re.sub(r'!\[\[.*?\.jpg.*?\]\]', '', content)
    content = re.sub(r'!\[\[.*?\.png.*?\]\]', '', content)
    return content

def extract_mood_level(content):
    """ä»æ€»ç»“éƒ¨åˆ†æå–å¿ƒæƒ…æŒ‡æ•°"""
    # å¯»æ‰¾æ€»ç»“éƒ¨åˆ†å‹¾é€‰çš„é€‰é¡¹
    summary_section = re.search(r'## æ€»ç»“(.*?)(?=^#|\Z)', content, re.DOTALL | re.MULTILINE)
    
    if not summary_section:
        return MOOD_LEVELS["æ— è®°å½•"]
    
    summary_content = summary_section.group(1)
    
    for mood, info in MOOD_LEVELS.items():
        if mood == "æ— è®°å½•":
            continue
        # æŸ¥æ‰¾æ˜¯å¦æœ‰è¯¥å¿ƒæƒ…é¡¹è¢«å‹¾é€‰
        if re.search(r'- \[x\]\s*' + re.escape(mood), summary_content):
            return info
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å‹¾é€‰çš„å¿ƒæƒ…ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ—©ç¡è®°å½•
    early_sleep = re.search(r'- \[x\]\s*æ˜¯å¦æ—©ç¡', summary_content)
    if early_sleep:
        return MOOD_LEVELS["ä¸€èˆ¬"]
    
    return MOOD_LEVELS["æ— è®°å½•"]

def extract_sections(content):
    """æå–å†…å®¹ä¸­çš„ä¸åŒéƒ¨åˆ†"""
    sections = {}
    
    # æå–"æµ“ç¼©çš„ä¸€å¤©"éƒ¨åˆ†
    day_summary_match = re.search(r'# æµ“ç¼©çš„ä¸€å¤©(.*?)(?=^#|\Z)', content, re.DOTALL | re.MULTILINE)
    if day_summary_match:
        sections['day_summary'] = day_summary_match.group(1).strip()
    
    # æå–å„ä¸ªäºŒçº§æ ‡é¢˜éƒ¨åˆ†
    section_pattern = r'## ([^\n]+)(.*?)(?=^##|\Z)'
    for match in re.finditer(section_pattern, content, re.DOTALL | re.MULTILINE):
        section_title = match.group(1).strip()
        section_content = match.group(2).strip()
        if section_content:
            sections[section_title] = section_content
    
    # æå–"æ˜æ—¥å¾…åŠ"éƒ¨åˆ†
    todo_match = re.search(r'# æ˜æ—¥å¾…åŠ(.*?)(?=^#|\Z)', content, re.DOTALL | re.MULTILINE)
    if todo_match:
        sections['tomorrow_todo'] = todo_match.group(1).strip()
    
    return sections

def get_summary_with_openai(content, api_key=None):
    """ä½¿ç”¨OpenAI APIç”Ÿæˆå†…å®¹æ‘˜è¦"""
    if not OPENAI_AVAILABLE:
        return None
    
    if api_key:
        openai.api_key = api_key
    elif 'OPENAI_API_KEY' in os.environ:
        openai.api_key = os.environ['OPENAI_API_KEY']
    else:
        print("è­¦å‘Š: æœªè®¾ç½®OpenAI APIå¯†é’¥ï¼Œæ— æ³•ä½¿ç”¨OpenAIç”Ÿæˆæ‘˜è¦")
        return None
    
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ—¥è®°åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»æ—¥è®°ä¸­æå–æœ€é‡è¦çš„äº‹ä»¶ã€æ€è€ƒå’Œæ„Ÿå—ï¼Œç”Ÿæˆç®€çŸ­çš„æ€»ç»“ã€‚"},
                {"role": "user", "content": f"è¯·ä»ä»¥ä¸‹æ—¥è®°å†…å®¹ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ª100å­—ä»¥å†…çš„ç®€çŸ­æ‘˜è¦ï¼Œçªå‡ºä½œè€…å½“å¤©çš„ä¸»è¦æ´»åŠ¨ã€æˆå°±ã€æ€è€ƒæˆ–å›°æ‰°ï¼š\n\n{content}"}
            ],
            max_tokens=200,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
        return None

def get_summary_with_ollama(content, model="llama3"):
    """ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹ç”Ÿæˆæ‘˜è¦"""
    if not OLLAMA_AVAILABLE:
        print("è­¦å‘Š: OllamaæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨Ollamaç”Ÿæˆæ‘˜è¦")
        return None
    
    try:
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ–‡ä»¶å­˜å‚¨è¯·æ±‚æ•°æ®
        payload = {
            "model": model,
            "prompt": f"ä½ æ˜¯ä¸€ä¸ªæ—¥è®°åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»æ—¥è®°ä¸­æå–æœ€é‡è¦çš„äº‹ä»¶å’Œæ„Ÿå—ã€‚è¯·ä»ä»¥ä¸‹æ—¥è®°å†…å®¹ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ª100å­—ä»¥å†…çš„ç®€çŸ­æ‘˜è¦ï¼Œçªå‡ºä½œè€…å½“å¤©çš„ä¸»è¦æ´»åŠ¨ã€æˆå°±æˆ–å›°æ‰°ï¼š\n\n{content}\n\næ€»ç»“ï¼š",
            "stream": False
        }
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            return result.get("response", "").strip()
        else:
            print(f"Ollama APIè°ƒç”¨å¤±è´¥: çŠ¶æ€ç  {response.status_code}")
            return None
    except Exception as e:
        print(f"Ollama APIè°ƒç”¨å¤±è´¥: {e}")
        return None

def extract_keywords_with_ollama(content, model="llama3"):
    """ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹æå–å…³é”®è¯"""
    if not OLLAMA_AVAILABLE:
        print("è­¦å‘Š: OllamaæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨Ollamaæå–å…³é”®è¯")
        return []
    
    try:
        payload = {
            "model": model,
            "prompt": f"è¯·ä»ä»¥ä¸‹æ—¥è®°å†…å®¹ä¸­æå–5ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼Œè¿™äº›å…³é”®è¯åº”è¯¥èƒ½ä»£è¡¨æ—¥è®°çš„æ ¸å¿ƒä¸»é¢˜å’Œæ´»åŠ¨ã€‚åªéœ€è¦åˆ—å‡ºå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸éœ€è¦å…¶ä»–è§£é‡Šï¼š\n\n{content}\n\nå…³é”®è¯ï¼š",
            "stream": False
        }
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            keywords_text = result.get("response", "").strip()
            # å¤„ç†è¿”å›çš„æ–‡æœ¬ï¼Œæå–å…³é”®è¯
            keywords = [k.strip() for k in re.split(r'[,ï¼Œã€\s]+', keywords_text) if k.strip()]
            return keywords[:5]  # é™åˆ¶æœ€å¤š5ä¸ªå…³é”®è¯
        else:
            print(f"Ollama APIè°ƒç”¨å¤±è´¥: çŠ¶æ€ç  {response.status_code}")
            return []
    except Exception as e:
        print(f"Ollama APIè°ƒç”¨å¤±è´¥: {e}")
        return []

def simple_summary(sections):
    """ä¸ä½¿ç”¨AIæ—¶ï¼Œä½¿ç”¨ç®€å•è§„åˆ™æå–æ‘˜è¦"""
    summary_parts = []
    
    # ä»ä¸€å¤©çš„æµ“ç¼©éƒ¨åˆ†æå–å·²å®Œæˆçš„ä»»åŠ¡
    if 'day_summary' in sections:
        tasks = re.findall(r'-\s*\[x\]\s*(.*?)$', sections['day_summary'], re.MULTILINE)
        if tasks:
            summary_parts.append("å®Œæˆäº†: " + ", ".join(tasks[:3]))
    
    # ä»å„éƒ¨åˆ†ä¸­æå–éæ¨¡æ¿çš„æ–‡æœ¬è¡Œï¼ˆä¸æ˜¯åˆ—è¡¨é¡¹ä¸”ä¸æ˜¯ç©ºè¡Œï¼‰
    for section_name, content in sections.items():
        if section_name not in ['day_summary', 'tomorrow_todo', 'æ€»ç»“']:
            # æå–éåˆ—è¡¨é¡¹ä¸”ä¸ä¸ºç©ºçš„è¡Œ
            lines = [line.strip() for line in content.split('\n') 
                    if line.strip() and not line.strip().startswith('-') and not line.strip().startswith('#')]
            if lines:
                # é€‰æ‹©æœ€é•¿çš„2-3è¡Œä½œä¸ºæœ‰ä¿¡æ¯é‡çš„å†…å®¹
                lines.sort(key=len, reverse=True)
                for line in lines[:2]:
                    if len(line) > 15:  # åªé€‰æ‹©è¶³å¤Ÿé•¿çš„è¡Œï¼Œå¯èƒ½åŒ…å«å®è´¨å†…å®¹
                        summary_parts.append(line)
    
    # å¦‚æœæå–çš„å†…å®¹ä¸è¶³ï¼Œå¯ä»¥ä»æ˜æ—¥å¾…åŠä¸­æ‰¾ä¸€äº›è®¡åˆ’
    if len(summary_parts) < 2 and 'tomorrow_todo' in sections:
        important_todos = re.findall(r'-\s*\[\s*\]\s*(.*?)$', sections['tomorrow_todo'], re.MULTILINE)
        if important_todos:
            summary_parts.append("è®¡åˆ’: " + important_todos[0])
    
    # ç»„åˆæ‘˜è¦ï¼Œç¡®ä¿é•¿åº¦åˆé€‚
    summary = " ".join(summary_parts)
    if len(summary) > 150:
        summary = summary[:147] + "..."
    
    return summary if summary else "æ—¥å¸¸è®°å½•"

def generate_tags(sections):
    """ä»å†…å®¹ä¸­ç”Ÿæˆæ ‡ç­¾"""
    tags = set()
    
    # æå–æ–‡ä¸­çš„#æ ‡ç­¾
    all_content = " ".join(sections.values())
    hashtags = re.findall(r'#(\w+)', all_content)
    tags.update(hashtags)
    
    # ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯ä½œä¸ºæ ‡ç­¾
    for section_name in sections.keys():
        if section_name not in ['day_summary', 'tomorrow_todo', 'æ€»ç»“']:
            # å°†éƒ¨åˆ†æ ‡é¢˜è¯ä¹ŸåŠ å…¥æ ‡ç­¾
            words = re.findall(r'(\w+)', section_name)
            tags.update(words)
    
    # å¦‚æœæ ‡ç­¾å¤ªå¤šï¼Œåªä¿ç•™å‡ ä¸ª
    return list(tags)[:5] if tags else []

def extract_simple_keywords(sections):
    """ä½¿ç”¨ç®€å•è§„åˆ™æå–å…³é”®è¯"""
    keywords = set()
    important_keywords = []
    
    # å…ˆå°è¯•ä»æµ“ç¼©çš„ä¸€å¤©ä¸­æå–å…³é”®è¯
    if 'day_summary' in sections:
        # ä»å‹¾é€‰çš„ä»»åŠ¡ä¸­æå–åè¯å’ŒåŠ¨è¯
        completed_tasks = re.findall(r'-\s*\[x\]\s*(.*?)$', sections['day_summary'], re.MULTILINE)
        for task in completed_tasks:
            # ç®€å•æå–ï¼Œä½¿ç”¨ä¸­æ–‡åˆ†è¯å¯èƒ½æ›´å‡†ç¡®ä½†å¢åŠ ä¾èµ–
            words = [w for w in re.findall(r'[\u4e00-\u9fa5a-zA-Z]+', task) if len(w) >= 2]
            important_keywords.extend(words[:2])  # æ¯ä¸ªä»»åŠ¡å–æœ€å¤š2ä¸ªè¯
    
    # ç„¶åä»å„ä¸ªéƒ¨åˆ†æå–å…³é”®è¯
    for section_name, content in sections.items():
        if section_name not in ['day_summary', 'tomorrow_todo', 'æ€»ç»“']:
            # éƒ¨åˆ†åç§°å¯èƒ½æ˜¯å…³é”®è¯
            if section_name not in ['ç”Ÿæ´»', 'è®ºæ–‡', 'ä»£ç é¢˜', 'å¼€æºé¡¹ç›®', 'app', 'é¡¹ç›®', 'å®ä¹ ', 'è‹±è¯­å­¦ä¹ è§†é¢‘', 'ä½“è‚²è¿åŠ¨', 'æ¬²æœ›']:
                keywords.add(section_name)
            
            # ä»å†…å®¹ä¸­æå–å…³é”®è¯
            # ç‰¹åˆ«å…³æ³¨ï¼šè¢«æ ‡è®°ä¸ºå®Œæˆçš„ä»»åŠ¡ã€å¸¦æœ‰æ ‡ç‚¹çš„å¥å­ï¼ˆå¯èƒ½æ˜¯é‡è¦é™ˆè¿°ï¼‰
            completed = re.findall(r'-\s*\[x\]\s*(.*?)$', content, re.MULTILINE)
            for item in completed:
                words = [w for w in re.findall(r'[\u4e00-\u9fa5a-zA-Z]+', item) if len(w) >= 2]
                keywords.update(words[:2])
    
    # åˆå¹¶é‡è¦å…³é”®è¯å’Œæ™®é€šå…³é”®è¯
    result = important_keywords + list(keywords)
    # å»é‡å¹¶é™åˆ¶æ•°é‡
    unique_result = []
    for word in result:
        if word not in unique_result and len(unique_result) < 5:
            unique_result.append(word)
    
    return unique_result

def process_markdown_file(file_path, use_openai=False, use_ollama=False, api_key=None, ollama_model="llama3"):
    """å¤„ç†å•ä¸ªMarkdownæ–‡ä»¶ï¼Œæå–æ—¶é—´çº¿äº‹ä»¶æ•°æ®"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except UnicodeDecodeError:
        try:
            with open(file_path, 'r', encoding='gbk') as f:
                content = f.read()
        except:
            print(f"æ— æ³•è¯»å–æ–‡ä»¶: {file_path}")
            return None
    
    # æå–æ—¥æœŸ
    date = extract_date_from_filename(file_path)
    if not date:
        date = extract_last_modified_date(content)
        if not date:
            # æ— æ³•ç¡®å®šæ—¥æœŸï¼Œè·³è¿‡
            print(f"æ— æ³•ç¡®å®šæ–‡ä»¶æ—¥æœŸ: {file_path}")
            return None
    
    # æ¸…ç†å†…å®¹
    cleaned_content = clean_markdown(content)
    
    # æå–ä¸åŒéƒ¨åˆ†
    sections = extract_sections(cleaned_content)
    
    # æå–å¿ƒæƒ…æŒ‡æ•°
    mood_info = extract_mood_level(cleaned_content)
    
    # ç”Ÿæˆæ‘˜è¦
    summary = None
    if use_openai:
        summary = get_summary_with_openai(cleaned_content, api_key)
    elif use_ollama:
        summary = get_summary_with_ollama(cleaned_content, ollama_model)
    
    if not summary:
        summary = simple_summary(sections)
    
    # æå–å…³é”®è¯
    keywords = []
    if use_ollama:
        keywords = extract_keywords_with_ollama(cleaned_content, ollama_model)
    
    if not keywords:
        keywords = extract_simple_keywords(sections) or generate_tags(sections)
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªé¢œè‰²ï¼Œä¼˜å…ˆä½¿ç”¨å¿ƒæƒ…å¯¹åº”çš„é¢œè‰²
    color = mood_info.get("color", random.choice(COLORS))
    
    # æ„å»ºæ—¶é—´çº¿äº‹ä»¶
    timeline_event = {
        "date": date,
        "title": f"æ—¥è®°: {date} {mood_info.get('emoji', '')}",
        "content": summary,
        "color": color,
        "mood_level": mood_info.get("level", 0),
        "tags": keywords
    }
    
    return timeline_event

def process_directory(directory_path, output_path, use_openai=False, use_ollama=False, api_key=None, ollama_model="llama3"):
    """å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰Markdownæ–‡ä»¶"""
    # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
    markdown_files = []
    for ext in ['*.md', '*.markdown']:
        pattern = os.path.join(directory_path, '**', ext)
        markdown_files.extend(glob.glob(pattern, recursive=True))
    
    print(f"æ‰¾åˆ° {len(markdown_files)} ä¸ªMarkdownæ–‡ä»¶")
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    timeline_events = []
    for file_path in markdown_files:
        print(f"å¤„ç†æ–‡ä»¶: {file_path}")
        event = process_markdown_file(file_path, use_openai, use_ollama, api_key, ollama_model)
        if event:
            timeline_events.append(event)
    
    # æŒ‰æ—¥æœŸæ’åº
    timeline_events.sort(key=lambda x: x["date"])
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(timeline_events, f, ensure_ascii=False, indent=2)
    
    print(f"å·²ç”Ÿæˆæ—¶é—´çº¿æ•°æ®ï¼ŒåŒ…å« {len(timeline_events)} ä¸ªäº‹ä»¶ï¼Œä¿å­˜è‡³: {output_path}")
    return timeline_events

def main():
    parser = argparse.ArgumentParser(description="ä»Markdownæ—¥è®°æ–‡ä»¶ç”Ÿæˆæ—¶é—´çº¿æ•°æ®")
    parser.add_argument("directory", help="åŒ…å«Markdownæ—¥è®°æ–‡ä»¶çš„ç›®å½•è·¯å¾„")
    parser.add_argument("--output", "-o", default="timeline_data.json", help="è¾“å‡ºçš„JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--use-openai", action="store_true", help="ä½¿ç”¨OpenAI APIç”Ÿæˆæ‘˜è¦")
    parser.add_argument("--use-ollama", action="store_true", help="ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹ç”Ÿæˆæ‘˜è¦å’Œå…³é”®è¯")
    parser.add_argument("--api-key", help="OpenAI APIå¯†é’¥")
    parser.add_argument("--ollama-model", default="llama3", help="ä½¿ç”¨çš„Ollamaæ¨¡å‹åç§° (é»˜è®¤: llama3)")
    
    args = parser.parse_args()
    
    if args.use_openai and not OPENAI_AVAILABLE:
        print("æ— æ³•ä½¿ç”¨OpenAI: æœªå®‰è£…openaiåŒ…ã€‚è¯·å®‰è£…åå†è¯•: pip install openai")
        args.use_openai = False
    
    if args.use_ollama and not OLLAMA_AVAILABLE:
        print("æ— æ³•ä½¿ç”¨Ollama: æœåŠ¡ä¸å¯ç”¨æˆ–æœªå“åº”ã€‚è¯·ç¡®ä¿Ollamaå·²å®‰è£…å¹¶è¿è¡Œã€‚")
        args.use_ollama = False
    
    process_directory(args.directory, args.output, args.use_openai, args.use_ollama, args.api_key, args.ollama_model)

if __name__ == "__main__":
    main() 